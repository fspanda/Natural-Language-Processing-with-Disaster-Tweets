{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Twitter has become an important communication channel in times of emergency.\n",
    "The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n",
    "\n",
    "But, it’s not always clear whether a person’s words are actually announcing a disaster. Take this example:\n",
    "\n",
    "\"Oh plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE\"\n",
    "\n",
    "\n",
    "The author explicitly uses the word “ABLAZE” but means it metaphorically. This is clear to a human right away, especially with the visual aid. But it’s less clear to a machine.\n",
    "\n",
    "I want to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t. I have access to a dataset of 10,000 tweets that were hand classified. The language used is Python, the framework is pytorch-lightning, transformers, and the base model is dabert. \n",
    "\n",
    "Model are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n",
    "\n",
    "\n",
    "Columns :\n",
    "id - a unique identifier for each tweet\n",
    "text - the text of the tweet\n",
    "location - the location the tweet was sent from (may be blank)\n",
    "keyword - a particular keyword from the tweet (may be blank)\n",
    "target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n",
    "\n",
    "This is Example Code:\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DabertTokenizer, DabertModel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "\n",
    "def get_sentence(c1, c2, c3):\n",
    "    if pd.isnull(c1):\n",
    "        c1 = \"\"\n",
    "    if pd.isnull(c2):\n",
    "        c2 = \"\"\n",
    "    if pd.isnull(c3):\n",
    "        c3 = \"\"\n",
    "    return f\"{c1} {c2} {c3}\"\n",
    "    \n",
    "def clean_text(data):\n",
    "    \"\"\"\n",
    "    input: data: a dataframe containing texts to be cleaned\n",
    "    return: the same dataframe with an added column of clean text\n",
    "    \"\"\"\n",
    "    data['clean_text'] = data['text'].str.lower()\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "    punctuations = list(punctuation)\n",
    "    clean_text = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for idx, row in enumerate(data['clean_text']):\n",
    "        split_text = row.split()\n",
    "        clean_text = [lemmatizer.lemmatize(word) for word in split_text if word not in stop_words and word not in punctuation]\n",
    "        \n",
    "        clean_text = ' '.join(clean_text)\n",
    "        data.loc[idx]['clean_text'] = clean_text\n",
    "    return data\n",
    "    \n",
    "class DisasterTweetClassifier(nn.Module):\n",
    "    def __init__(self, model_name=\"dabert-base-model\",use_other_features=False,clean_text=False):\n",
    "        super().__init__()\n",
    "        self.num_classes = 2\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)     \n",
    "        self.data = pd.read_csv(\"path/to/training/data.csv\")\n",
    "        if use_other_features:\n",
    "            self.data[\"text\"]=self.data.apply(lambda x: get_sentence(x[\"keyword\"],x[\"location\"],x[\"text\"]),axis=1)\n",
    "        if clean_text:\n",
    "            self.data=clean_text(self.data)\n",
    "        self.train_data,self.val_data=train_test_split(self.data,test_size=0.2,random_state=42)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        \n",
    "\n",
    "    def call(self, input_ids):\n",
    "        # Forward all of the input data through the model\n",
    "        logits = self.model(input_ids)[0]\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        return self.call(input_ids)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Training step\n",
    "        input_ids, labels = batch\n",
    "        logits = self.forward(input_ids)\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Validation step\n",
    "        input_ids, labels = batch\n",
    "        logits = self.forward(input_ids)\n",
    "        val_loss = self.loss_fn(logits, labels)\n",
    "        return {\"val_loss\": val_loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # Validation end\n",
    "        avg_val_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        return {\"avg_val_loss\": avg_val_loss}\n",
    "\n",
    "               \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Return a DataLoader for the training set\n",
    "        # This will be used to iterate over the training data during training\n",
    "\n",
    "        \n",
    "        input_ids = [self.tokenizer.encode(text) for text in self.train_data[\"text\"]]   \n",
    "        # Create PyTorch tensors for the input and target variables\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        labels = torch.tensor(self.train_data[\"target\"])\n",
    "        train_data = torch.utils.data.TensorDataset(input_ids, labels)\n",
    "        train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # Return a DataLoader for the validation set\n",
    "        # This will be used to iterate over the validation data during training\n",
    "     \n",
    "        input_ids = [self.tokenizer.encode(text) for text in self.val_data[\"text\"]]\n",
    "   \n",
    "\n",
    "        # Create PyTorch tensors for the input and target variables\n",
    "        input_ids = torch.tensor(input_ids) \n",
    "        labels = torch.tensor(self.val_data[\"target\"])\n",
    "        val_data = torch.utils.data.TensorDataset(input_ids,labels)\n",
    "        val_dataloader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "        return val_dataloader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You don't have to make the explanation short or not. How should I make the Dataloader part?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DabertTokenizer, DabertModel\n",
    "\n",
    "class DisasterTweetClassifier(pl.LightningModule):\n",
    "    def __init__(self, num_classes=2,test_data=None,train_data=None,val_data=None):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.tokenizer = DabertTokenizer.from_pretrained(\"dabert-base-model\")\n",
    "        self.model = DabertModel.from_pretrained(\"dabert-base-model\")\n",
    "        self.train_data=train_data\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Forward pass of the model\n",
    "        logits = self.model(input_ids)[0]\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Training step\n",
    "        input_ids, labels = batch\n",
    "        logits = self.forward(input_ids)\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Validation step\n",
    "        input_ids, labels = batch\n",
    "        logits = self.forward(input_ids)\n",
    "        val_loss = self.loss_fn(logits, labels)\n",
    "        return {\"val_loss\": val_loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # Validation end\n",
    "        avg_val_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        return {\"avg_val_loss\": avg_val_loss}\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Load and prepare the data\n",
    "        # This could involve reading the data from a file,\n",
    "        # preprocessing it, and creating PyTorch tensors from it\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Return a DataLoader for the training set\n",
    "        train_dataloader = DataLoader(self.train_data, batch_size=32, shuffle=True)\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # Return a DataLoader for the validation set\n",
    "        val_dataloader = DataLoader(self.val_data, batch_size=32, shuffle=False)\n",
    "        return val_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'Python 3.8.0 64-bit'(으)로 셀을 실행하려면 ipykernel 패키지가 필요합니다.\n",
      "\u001b[1;31m다음 명령어를 실행하여 Python 환경에 'ipykernel'을(를) 설치합니다. \n",
      "\u001b[1;31m 명령: '/usr/bin/python3.8 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Load the training data from a file or database\n",
    "data = pd.read_csv(\"path/to/training/data.csv\")\n",
    "\n",
    "# Tokenize the text, location, and keyword columns using the Dabert tokenizer\n",
    "tokenizer = DabertTokenizer.from_pretrained(\"dabert-base-model\")\n",
    "input_ids = [tokenizer.encode(text) for text in data[\"text\"]]\n",
    "locations = [tokenizer.encode(location) for location in data[\"location\"]]\n",
    "keywords = [tokenizer.encode(keyword) for keyword in data[\"keyword\"]]\n",
    "\n",
    "# Create PyTorch tensors for the input and target variables\n",
    "input_ids = torch.tensor(input_ids)\n",
    "locations = torch.tensor(locations)\n",
    "keywords = torch.tensor(keywords)\n",
    "labels = torch.tensor(data[\"target\"])\n",
    "\n",
    "# Combine the input and target tensors into a single dataset\n",
    "train_data = torch.utils.data.TensorDataset(input_ids, locations, keywords, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisasterTweetClassifier(nn.Module):\n",
    "    def __init__(self, model_name=\"dabert-base-model\"):\n",
    "        super().__init__()\n",
    "        self.num_classes = 2\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)     \n",
    "        self.data = pd.read_csv(\"path/to/training/data.csv\")\n",
    "        self.train_data,self.val_data=train_test_split(self.data,test_size=0.2,random_state=42)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Forward pass of the model\n",
    "        logits = self.model(input_ids)[0]\n",
    "        return logits\n",
    "\n",
    "    def call(self, input_ids):\n",
    "        # Call method to forward input through the model\n",
    "        logits = self.forward(input_ids)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Training step\n",
    "        input_ids, labels = batch\n",
    "        logits = self.forward(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DabertTokenizer' from 'transformers' (/home/ybi/anaconda3/envs/SIMCSE/lib/python3.8/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m DabertTokenizer, DabertModel\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DabertTokenizer' from 'transformers' (/home/ybi/anaconda3/envs/SIMCSE/lib/python3.8/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "\n",
    "def get_sentence(c1, c2, c3):\n",
    "    if pd.isnull(c1):\n",
    "        c1 = \"\"\n",
    "    if pd.isnull(c2):\n",
    "        c2 = \"\"\n",
    "    if pd.isnull(c3):\n",
    "        c3 = \"\"\n",
    "    return f\"{c1} {c2} {c3}\"\n",
    "    \n",
    "def clean_text(data):\n",
    "    \"\"\"\n",
    "    input: data: a dataframe containing texts to be cleaned\n",
    "    return: the same dataframe with an added column of clean text\n",
    "    \"\"\"\n",
    "    data['clean_text'] = data['text'].str.lower()\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "    punctuations = list(punctuation)\n",
    "    clean_text = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for idx, row in enumerate(data['clean_text']):\n",
    "        split_text = row.split()\n",
    "        clean_text = [lemmatizer.lemmatize(word) for word in split_text if word not in stop_words and word not in punctuation]\n",
    "        \n",
    "        clean_text = ' '.join(clean_text)\n",
    "        data.loc[idx]['clean_text'] = clean_text\n",
    "    return data\n",
    "    \n",
    "class DisasterTweetClassifier(nn.Module):\n",
    "    def __init__(self, model_name=\"dabert-base-model\",use_other_features=False,clean_text=False):\n",
    "        super().__init__()\n",
    "        self.num_classes = 2\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)     \n",
    "        self.data = pd.read_csv(\"/home/ybi/study/Kaggle/Natural-Language-Processing-with-Disaster-Tweets/Data/train.csv\")\n",
    "        if use_other_features:\n",
    "            self.data[\"text\"]=self.data.apply(lambda x: get_sentence(x[\"keyword\"],x[\"location\"],x[\"text\"]),axis=1)\n",
    "        if clean_text:\n",
    "            self.data=clean_text(self.data)\n",
    "        self.train_data,self.val_data=train_test_split(self.data,test_size=0.2,random_state=42)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        \n",
    "\n",
    "    def call(self, input_ids):\n",
    "        # Forward all of the input data through the model\n",
    "        logits = self.model(input_ids)[0]\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        return self.call(input_ids)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Training step\n",
    "        input_ids, labels = batch\n",
    "        logits = self.forward(input_ids)\n",
    "        loss = self.loss_fn(logits, labels)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Validation step\n",
    "        input_ids, labels = batch\n",
    "        logits = self.forward(input_ids)\n",
    "        val_loss = self.loss_fn(logits, labels)\n",
    "        return {\"val_loss\": val_loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # Validation end\n",
    "        avg_val_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        return {\"avg_val_loss\": avg_val_loss}\n",
    "\n",
    "               \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Return a DataLoader for the training set\n",
    "        # This will be used to iterate over the training data during training\n",
    "\n",
    "        \n",
    "        input_ids = [self.tokenizer.encode(text) for text in self.train_data[\"text\"]]   \n",
    "        # Create PyTorch tensors for the input and target variables\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        labels = torch.tensor(self.train_data[\"target\"])\n",
    "        train_data = torch.utils.data.TensorDataset(input_ids, labels)\n",
    "        train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # Return a DataLoader for the validation set\n",
    "        # This will be used to iterate over the validation data during training\n",
    "     \n",
    "        input_ids = [self.tokenizer.encode(text) for text in self.val_data[\"text\"]]\n",
    "   \n",
    "\n",
    "        # Create PyTorch tensors for the input and target variables\n",
    "        input_ids = torch.tensor(input_ids) \n",
    "        labels = torch.tensor(self.val_data[\"target\"])\n",
    "        val_data = torch.utils.data.TensorDataset(input_ids,labels)\n",
    "        val_dataloader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "        return val_dataloader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the DisasterTweetClassifier model\n",
    "model = DisasterTweetClassifier()\n",
    "\n",
    "# Instantiate a PyTorch Lightning Trainer\n",
    "trainer = pl.Trainer(gpus=1)\n",
    "\n",
    "# Train the model using the fit method\n",
    "trainer.fit(model, num_epochs=10, learning_rate=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('SIMCSE')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f616f51d52dfa83f14049184699891d0d65a5dc3abc42b031509921c6e48373a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
